{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=150 src=https://raw.githubusercontent.com/autonomio/signs/master/logo.png><center><font size=3>Signs is a set of tools for text preparation, vectorization and processing. Below is provided a set of examples that cover many of the commonly used workflows. </font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signs as signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will read some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dedomena as da\n",
    "docs = da.apis.pubmed('cervical cancer', 500, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation | `signs.Transform()`\n",
    "\n",
    "Then next, let's clean up the data. `signs.Transform()` allows the systematic creation of all important data formats from a single class object. If `clean=True` then the following preprocessing tasks will be performed:\n",
    "\n",
    "- force lower case\n",
    "- remove urls\n",
    "- remove emojis\n",
    "- remove punctuation\n",
    "- remove linebreaks\n",
    "- remove leading and traing whitespace\n",
    "- comprehensive stopwords filtering\n",
    "- decode from binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = signs.Transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the original docs\n",
    "docs.docs()\n",
    "\n",
    "# return the original docs cleaned\n",
    "docs.docs(True)\n",
    "\n",
    "# return original docs but flattened\n",
    "docs.docs_flat()\n",
    "\n",
    "# return original docs flattened and clean\n",
    "docs.docs_flat(True)\n",
    "\n",
    "# return original docs in a single string blob\n",
    "docs.docs_string()\n",
    "\n",
    "# return original docs in single string blob cleaned\n",
    "docs.docs_string(True)\n",
    "\n",
    "# return tokenized version of docs\n",
    "docs.tokens()\n",
    "\n",
    "# return tokenized version cleaned\n",
    "docs.tokens(True)\n",
    "\n",
    "# return tokenized and flattend\n",
    "docs.tokens_flat()\n",
    "\n",
    "# return tokenized and flattened clean\n",
    "docs.tokens_flat(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the the following examples will utilize one of these data formats by calling the class object `docs` we have created above. It's better to always ingest the original docs into `signs.Transform` to minimize overhead while being sure of format compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Statistics | `signs.Stopwords()`\n",
    "\n",
    "**Signs** allows stopword removal against an arbitrary sized list of stopwords in roughly 10,000 documents per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = signs.Stopwords(docs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Statistics | `signs.Describe()`\n",
    "**Signs** provides common text analytics functionalities under the Describe() class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read documents\n",
    "desc = signs.Describe(docs.tokens_flat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.get_counts()\n",
    "desc.get_gram_counts(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grams | `signs.Grams()`\n",
    "**Signs** provides access to ngrams and skipgrams through `signs.Grams()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "signs.Grams(docs.tokens_flat()).ngrams(2)\n",
    "\n",
    "# trigrams\n",
    "signs.Grams(docs.tokens_flat()).ngrams(3)\n",
    "\n",
    "# trigram with 2-step skipgram\n",
    "signs.Grams(docs.tokens_flat()).ngrams(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippets of text | `signs.Verbatims()`\n",
    "Another helpful feature is exracting verbatims based on a keyword and boundary through `signs.Verbatims()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs.Verbatims(docs.tokens_flat()).verbatims('cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Vectors | `signs.TrainDoc2Vec()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then train the Doc2Vec model\n",
    "model, train_corpus = signs.TrainDoc2Vec(docs.tokens()[:450])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Document Similarity | `signs.DocSimilarity()`\n",
    "\n",
    "There are several document similarity options available. Examples for each are provided below.\n",
    "\n",
    "- similarity matrix for seen documents\n",
    "- similarity matrix for unseen documents\n",
    "- similarity between a single unseen document and seen docs\n",
    "- spatial distance between two documents, seen or unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = signs.DocSimilarity(model, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options for getting the similarities:\n",
    "\n",
    "- `similar_docs()` for any document to all training documents\n",
    "- `spatial_distance()` for any document to any document\n",
    "- `seen_matrix()` for a 2d similarity matrix for all training documents\n",
    "- `unseen_matrix()` for a 2d similarity matrix for any set of documents\n",
    "\n",
    "Note that `unseen_matrix()` might take time as the matrix grows. Example of use as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims.similar_docs(docs.tokens()[1])\n",
    "\n",
    "sims.spatial_distance(doc1=docs.tokens()[451],\n",
    "                      doc2=docs.tokens()[452])\n",
    "\n",
    "sims.seen_matrix()\n",
    "\n",
    "sims.unseen_matrix(docs.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a method for previewing the most and least similar documents as a reference. This is done with `sims.preview_results()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
